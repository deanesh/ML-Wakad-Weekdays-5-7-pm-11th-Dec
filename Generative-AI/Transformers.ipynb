{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7d3ac4",
   "metadata": {},
   "source": [
    "#### Transformers \n",
    "\n",
    "Transformers are a deep learning architecture that uses self-attention to understand relationships between all parts of input data at once, instead of processing it sequentially.\n",
    "\n",
    "They capture context and meaning efficiently\n",
    "\n",
    "Enable parallel processing (faster training)\n",
    "\n",
    "Form the backbone of LLMs like GPT, BERT, and multimodal GenAI models\n",
    "\n",
    "Example: GPT uses transformers to generate coherent text by attending to relevant words in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78645e54",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Wakad-Weekdays-5-7-pm-11th-Dec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
